{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "\n",
    "from base_code.models.poc import NeuralNetwork as NN\n",
    "from base_code.datasets import DryBeansDataset as DBD\n",
    "from base_code.train_test import train, test\n",
    "\n",
    "from base_code.preprocessing.nomalize_standarizer import pipeline as normalize_standarize\n",
    "from base_code.dataloaders.base import ContinualLearningDataLoader as DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DBD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_task_range(0, 2).labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = normalize_standarize(dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pipeline.transform(dataset.features)\n",
    "dataset.features = X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Netwrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(features_shape=dataset.features_shape, output_shape=dataset.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "tensor([[ 5.6540e-02, -1.1788e-02,  2.0252e-01, -5.7454e-01,  1.0717e-01,\n",
      "         -3.3371e-02, -5.0533e-02, -2.7293e-01, -7.5066e-01, -3.6967e-01,\n",
      "         -5.6889e-01, -5.4597e-01, -3.0234e-01, -7.1445e-01, -6.6991e-01,\n",
      "         -3.8234e-01],\n",
      "        [ 2.7054e-01,  4.0839e-01, -1.6211e-01,  1.0891e+00, -2.4844e-01,\n",
      "         -3.1871e-01, -2.7831e-01,  5.6460e-01,  7.0248e-01,  5.1522e-01,\n",
      "          6.3810e-01,  8.4676e-01,  1.8342e-01,  1.1223e+00,  1.1244e+00,\n",
      "          5.1986e-01],\n",
      "        [ 2.7180e-01,  7.7477e-01,  6.9405e-01,  8.6585e-01,  6.7975e-01,\n",
      "          8.2399e-01, -2.9159e-01,  8.6385e-01,  8.0996e-01,  8.6386e-01,\n",
      "          9.1749e-01,  8.7278e-01,  7.9442e-01,  7.9750e-01,  8.4753e-01,\n",
      "          8.6675e-01],\n",
      "        [ 5.0860e-01,  1.0880e+00,  1.0848e+00,  1.0399e+00,  1.1480e+00,\n",
      "          1.2810e+00, -5.3968e-01,  1.1537e+00,  1.0832e+00,  1.2203e+00,\n",
      "          1.2435e+00,  1.1543e+00,  1.2533e+00,  1.0481e+00,  1.0628e+00,\n",
      "          1.2213e+00],\n",
      "        [ 6.5125e-01,  5.6514e-01,  4.9558e-01,  7.2445e-01,  4.4922e-01,\n",
      "          5.9307e-01, -6.6566e-01,  6.8618e-01,  7.9080e-01,  6.5791e-01,\n",
      "          7.3886e-01,  6.8061e-01,  5.4538e-01,  6.0338e-01,  6.7701e-01,\n",
      "          6.5449e-01],\n",
      "        [-1.9765e+00, -4.4509e-02,  3.9547e-01, -6.6445e-01,  2.9863e-01,\n",
      "          3.8357e-02,  1.9754e+00, -2.7874e-01, -5.7440e-01, -3.8691e-01,\n",
      "         -5.4554e-01, -6.0624e-01, -2.3202e-01, -7.9401e-01, -7.7000e-01,\n",
      "         -4.0054e-01],\n",
      "        [ 6.7324e-01, -1.5873e-03,  2.4455e-01, -1.8010e-01,  1.5885e-01,\n",
      "          1.5981e-01, -6.7048e-01,  9.1964e-03, -1.9886e-02, -8.3307e-02,\n",
      "         -9.6697e-02, -2.0649e-01, -9.1905e-02, -3.9451e-01, -3.0224e-01,\n",
      "         -9.6338e-02],\n",
      "        [ 4.3103e-01,  9.2792e-01,  1.4142e-01,  1.7960e+00,  6.3946e-02,\n",
      "         -3.8307e-01, -4.5601e-01,  1.0973e+00,  1.2235e+00,  1.1492e+00,\n",
      "          1.2902e+00,  1.6343e+00,  7.5041e-01,  2.3181e+00,  2.0625e+00,\n",
      "          1.1494e+00],\n",
      "        [-6.6007e-01, -2.4462e+00, -2.3947e+00, -1.8584e+00, -2.0612e+00,\n",
      "         -1.9530e+00,  7.1965e-01, -2.2188e+00, -1.7999e+00, -1.9010e+00,\n",
      "         -1.7143e+00, -1.7445e+00, -1.6861e+00, -1.3531e+00, -1.5842e+00,\n",
      "         -1.8982e+00],\n",
      "        [ 1.1448e-01,  5.7355e-01,  4.7150e-01,  4.7302e-01,  4.1873e-01,\n",
      "          5.3795e-01, -1.2660e-01,  5.1898e-01,  3.0415e-01,  4.6244e-01,\n",
      "          3.8236e-01,  4.3161e-01,  3.9902e-01,  2.8353e-01,  3.8193e-01,\n",
      "          4.6040e-01]])\n",
      "tensor([[0.1222, 0.1555, 0.1528, 0.1307, 0.1531, 0.1272, 0.1585],\n",
      "        [0.1153, 0.1655, 0.1603, 0.1322, 0.1567, 0.1260, 0.1439],\n",
      "        [0.1166, 0.1577, 0.1646, 0.1286, 0.1571, 0.1241, 0.1514],\n",
      "        [0.1133, 0.1580, 0.1718, 0.1244, 0.1570, 0.1235, 0.1520],\n",
      "        [0.1168, 0.1581, 0.1622, 0.1307, 0.1571, 0.1258, 0.1494],\n",
      "        [0.1233, 0.1553, 0.1499, 0.1255, 0.1614, 0.1232, 0.1614],\n",
      "        [0.1244, 0.1576, 0.1503, 0.1333, 0.1521, 0.1256, 0.1567],\n",
      "        [0.1046, 0.1690, 0.1771, 0.1259, 0.1571, 0.1261, 0.1402],\n",
      "        [0.1078, 0.1676, 0.1789, 0.1078, 0.1602, 0.1242, 0.1535],\n",
      "        [0.1214, 0.1556, 0.1553, 0.1328, 0.1554, 0.1247, 0.1548]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jesusoyanedel/Documents/USM/tt-2/notebooks/stochastic_gradient_descent.ipynb Cell 19\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jesusoyanedel/Documents/USM/tt-2/notebooks/stochastic_gradient_descent.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jesusoyanedel/Documents/USM/tt-2/notebooks/stochastic_gradient_descent.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jesusoyanedel/Documents/USM/tt-2/notebooks/stochastic_gradient_descent.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train(model, train_dataloader, loss_fn, optimizer)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jesusoyanedel/Documents/USM/tt-2/notebooks/stochastic_gradient_descent.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     test(model, train_dataloader, loss_fn)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jesusoyanedel/Documents/USM/tt-2/notebooks/stochastic_gradient_descent.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/USM/tt-2/notebooks/../base_code/train_test.py:41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m     \u001b[39m# Compute prediction and loss\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     pred \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m---> 41\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(pred, y, model)\n\u001b[1;32m     43\u001b[0m     \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/USM/tt-2/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/USM/tt-2/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(model, train_dataloader, loss_fn, optimizer)\n",
    "    test(model, train_dataloader, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.276212  [   10/ 2027]\n",
      "loss: 1.297114  [ 1010/ 2027]\n",
      "loss: 1.187896  [ 2010/ 2027]\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.201651 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.204704  [   10/ 2027]\n",
      "loss: 1.175839  [ 1010/ 2027]\n",
      "loss: 1.219845  [ 2010/ 2027]\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 1.185685 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.166602  [   10/ 2027]\n",
      "loss: 1.170341  [ 1010/ 2027]\n",
      "loss: 1.174511  [ 2010/ 2027]\n",
      "Test Error: \n",
      " Accuracy: 99.8%, Avg loss: 1.178281 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.179792  [   10/ 2027]\n",
      "loss: 1.168353  [ 1010/ 2027]\n",
      "loss: 1.172774  [ 2010/ 2027]\n",
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 1.174184 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.165649  [   10/ 2027]\n",
      "loss: 1.165880  [ 1010/ 2027]\n",
      "loss: 1.173934  [ 2010/ 2027]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 1.171874 \n",
      "\n",
      "Task 2\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.112403  [   10/ 1322]\n",
      "loss: 2.112149  [ 1010/ 1322]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 1.541163 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.102550  [   10/ 1322]\n",
      "loss: 2.092286  [ 1010/ 1322]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 1.534869 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.099954  [   10/ 1322]\n",
      "loss: 1.842562  [ 1010/ 1322]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 1.220457 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.355424  [   10/ 1322]\n",
      "loss: 1.176173  [ 1010/ 1322]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 1.191743 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.181103  [   10/ 1322]\n",
      "loss: 1.169820  [ 1010/ 1322]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 1.190983 \n",
      "\n",
      "Task 3\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.165422  [   10/  522]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 1.322020 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.165422  [   10/  522]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 1.322345 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.165421  [   10/  522]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 1.322018 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.165422  [   10/  522]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 1.322035 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.165421  [   10/  522]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 1.322020 \n",
      "\n",
      "Task 4\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.164654  [   10/ 1630]\n",
      "loss: 2.163928  [ 1010/ 1630]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 1.570972 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.161106  [   10/ 1630]\n",
      "loss: 2.162145  [ 1010/ 1630]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 1.570907 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.163572  [   10/ 1630]\n",
      "loss: 2.164357  [ 1010/ 1630]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 1.570819 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.160835  [   10/ 1630]\n",
      "loss: 2.154101  [ 1010/ 1630]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.570222 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.154273  [   10/ 1630]\n",
      "loss: 1.187674  [ 1010/ 1630]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 1.508206 \n",
      "\n",
      "Task 5\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.955737  [   10/ 1928]\n",
      "loss: 1.236516  [ 1010/ 1928]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: 1.606281 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.190040  [   10/ 1928]\n",
      "loss: 1.273422  [ 1010/ 1928]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 1.623141 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.168507  [   10/ 1928]\n",
      "loss: 1.168737  [ 1010/ 1928]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.626410 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.172681  [   10/ 1928]\n",
      "loss: 1.166759  [ 1010/ 1928]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.627724 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.236825  [   10/ 1928]\n",
      "loss: 1.170189  [ 1010/ 1928]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.628125 \n",
      "\n",
      "Task 6\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.105368  [   10/ 2636]\n",
      "loss: 2.050176  [ 1010/ 2636]\n",
      "loss: 1.814025  [ 2010/ 2636]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.581925 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.490117  [   10/ 2636]\n",
      "loss: 1.282908  [ 1010/ 2636]\n",
      "loss: 1.201879  [ 2010/ 2636]\n",
      "Test Error: \n",
      " Accuracy: 26.8%, Avg loss: 1.853275 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.192948  [   10/ 2636]\n",
      "loss: 1.187344  [ 1010/ 2636]\n",
      "loss: 1.183368  [ 2010/ 2636]\n",
      "Test Error: \n",
      " Accuracy: 26.2%, Avg loss: 1.887329 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.186135  [   10/ 2636]\n",
      "loss: 1.172725  [ 1010/ 2636]\n",
      "loss: 1.172441  [ 2010/ 2636]\n",
      "Test Error: \n",
      " Accuracy: 26.2%, Avg loss: 1.895598 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.172663  [   10/ 2636]\n",
      "loss: 1.173614  [ 1010/ 2636]\n",
      "loss: 1.168674  [ 2010/ 2636]\n",
      "Test Error: \n",
      " Accuracy: 26.2%, Avg loss: 1.898831 \n",
      "\n",
      "Task 7\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.165407  [   10/ 3546]\n",
      "loss: 2.165038  [ 1010/ 3546]\n",
      "loss: 2.165250  [ 2010/ 3546]\n",
      "loss: 2.165350  [ 3010/ 3546]\n",
      "Test Error: \n",
      " Accuracy: 19.4%, Avg loss: 1.968324 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.165366  [   10/ 3546]\n",
      "loss: 2.165125  [ 1010/ 3546]\n",
      "loss: 2.165105  [ 2010/ 3546]\n",
      "loss: 2.165322  [ 3010/ 3546]\n",
      "Test Error: \n",
      " Accuracy: 19.4%, Avg loss: 1.968241 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.165301  [   10/ 3546]\n",
      "loss: 2.164597  [ 1010/ 3546]\n",
      "loss: 2.165359  [ 2010/ 3546]\n",
      "loss: 2.165192  [ 3010/ 3546]\n",
      "Test Error: \n",
      " Accuracy: 19.4%, Avg loss: 1.968152 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.165389  [   10/ 3546]\n",
      "loss: 2.165353  [ 1010/ 3546]\n",
      "loss: 2.165312  [ 2010/ 3546]\n",
      "loss: 2.165298  [ 3010/ 3546]\n",
      "Test Error: \n",
      " Accuracy: 19.4%, Avg loss: 1.967399 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.165152  [   10/ 3546]\n",
      "loss: 2.165005  [ 1010/ 3546]\n",
      "loss: 2.165020  [ 2010/ 3546]\n",
      "loss: 2.165182  [ 3010/ 3546]\n",
      "Test Error: \n",
      " Accuracy: 19.4%, Avg loss: 1.967948 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for task_id in range(train_dataloader.get_tasks_length()):\n",
    "    print(f\"Task {task_id+1}\\n-------------------------------\")\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(model, train_dataloader.get_task(task_id), loss_fn, optimizer)\n",
    "        test(model, train_dataloader.get_task_range(0, task_id+1), loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
