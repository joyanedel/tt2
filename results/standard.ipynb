{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../base_code')\n",
    "\n",
    "from base_code.constants import SAVED_METRICS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A-GEM Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agem_data = pickle.load(open(SAVED_METRICS_PATH / 'standard' / 'agem.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly plot\n",
    "fig = px.line(\n",
    "    pd.DataFrame(agem_data[\"accuracies\"]),\n",
    "    title=\"Accuracy vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Accuracy\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    range_y=[0, 1],\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis=dict(tickmode=\"linear\", tick0=0, dtick=1),\n",
    "    yaxis=dict(tickmode=\"linear\", tick0=0, dtick=0.1),\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forgetting Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (key, value) in enumerate(agem_data[\"forgettings\"].items()):\n",
    "    agem_data[\"forgettings\"][key] = [float(\"nan\")] * i + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    pd.DataFrame(agem_data[\"forgettings\"]),\n",
    "    title=\"Forgetting vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Forgetting\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_xaxes(type=\"category\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Transfer (BWT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (key, value) in enumerate(agem_data[\"bwt\"].items()):\n",
    "    agem_data[\"bwt\"][key] = [float(\"nan\")] * i + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    pd.DataFrame(agem_data[\"bwt\"]),\n",
    "    title=\"BWT vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"BWT\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_xaxes(type=\"category\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    pd.DataFrame(agem_data[\"losses\"]),\n",
    "    title=\"Losses vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Loss\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_xaxes(type=\"category\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    agem_data[\"train_cpu_usage\"],\n",
    "    title=\"CPU Usage vs Mini Batch\",\n",
    "    labels=dict(index=\"Mini Batch\", value=\"Value\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Time Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    agem_data[\"train_time_epoch\"],\n",
    "    title=\"Time per Epoch vs Epoch\",\n",
    "    labels=dict(index=\"Epoch\", value=\"Value\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "fig.update_xaxes(type=\"category\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agem_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_data = pickle.load(open(SAVED_METRICS_PATH / 'standard' / 'cumulative.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly plot\n",
    "fig = px.line(\n",
    "    pd.DataFrame(cumulative_data[\"accuracies\"]),\n",
    "    title=\"Accuracy vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Accuracy\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    range_y=[0, 1],\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis=dict(tickmode=\"linear\", tick0=0, dtick=1),\n",
    "    yaxis=dict(tickmode=\"linear\", tick0=0, dtick=0.1),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EWC Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewc_data = pickle.load(open(SAVED_METRICS_PATH / 'standard' / 'ewc.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly plot\n",
    "fig = px.line(\n",
    "    pd.DataFrame(ewc_data[\"accuracies\"]),\n",
    "    title=\"Accuracy vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Accuracy\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    range_y=[0, 1],\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis=dict(tickmode=\"linear\", tick0=0, dtick=1),\n",
    "    yaxis=dict(tickmode=\"linear\", tick0=0, dtick=0.1),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEM Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gem_data = pickle.load(open(SAVED_METRICS_PATH / 'standard' / 'gem.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly plot\n",
    "fig = px.line(\n",
    "    pd.DataFrame(gem_data[\"accuracies\"]),\n",
    "    title=\"Accuracy vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Accuracy\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    range_y=[0, 1],\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis=dict(tickmode=\"linear\", tick0=0, dtick=1),\n",
    "    yaxis=dict(tickmode=\"linear\", tick0=0, dtick=0.1),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LWF Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lwf_data = pickle.load(open(SAVED_METRICS_PATH / 'standard' / 'lwf.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly plot\n",
    "fig = px.line(\n",
    "    pd.DataFrame(lwf_data[\"accuracies\"]),\n",
    "    title=\"Accuracy vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Accuracy\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    range_y=[0, 1],\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis=dict(tickmode=\"linear\", tick0=0, dtick=1),\n",
    "    yaxis=dict(tickmode=\"linear\", tick0=0, dtick=0.1),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIR Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mir_data = pickle.load(open(SAVED_METRICS_PATH / 'standard' / 'mir.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly plot\n",
    "fig = px.line(\n",
    "    pd.DataFrame(mir_data[\"accuracies\"]),\n",
    "    title=\"Accuracy vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Accuracy\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    range_y=[0, 1],\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis=dict(tickmode=\"linear\", tick0=0, dtick=1),\n",
    "    yaxis=dict(tickmode=\"linear\", tick0=0, dtick=0.1),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive = pickle.load(open(SAVED_METRICS_PATH / 'standard' / 'naive.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly plot\n",
    "fig = px.line(\n",
    "    pd.DataFrame(naive[\"accuracies\"]),\n",
    "    title=\"Accuracy vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Accuracy\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    range_y=[0, 1],\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis=dict(tickmode=\"linear\", tick0=0, dtick=1),\n",
    "    yaxis=dict(tickmode=\"linear\", tick0=0, dtick=0.1),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_data = pickle.load(open(SAVED_METRICS_PATH / 'standard' / 'offline.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_accuracies = {\n",
    "    key: value\n",
    "    for key, [value] in offline_data[\"accuracies\"].items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly barplot\n",
    "fig = px.bar(\n",
    "    pd.DataFrame(offline_accuracies, index=[\"Accuracy\"]).T,\n",
    "    title=\"Accuracy vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Accuracy\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    y=\"Accuracy\",\n",
    "    range_y=[0.92, 0.932],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_1_data = pickle.load(open(SAVED_METRICS_PATH / 'standard' / 'proposal_1.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly plot\n",
    "fig = px.line(\n",
    "    pd.DataFrame(proposal_1_data[\"accuracies\"]),\n",
    "    title=\"Accuracy vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Accuracy\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    range_y=[0, 1],\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis=dict(tickmode=\"linear\", tick0=0, dtick=1),\n",
    "    yaxis=dict(tickmode=\"linear\", tick0=0, dtick=0.1),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_2_data = pickle.load(open(SAVED_METRICS_PATH / 'standard' / 'proposal_2.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly plot\n",
    "fig = px.line(\n",
    "    pd.DataFrame(proposal_2_data[\"accuracies\"]),\n",
    "    title=\"Accuracy vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Accuracy\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    range_y=[0, 1],\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis=dict(tickmode=\"linear\", tick0=0, dtick=1),\n",
    "    yaxis=dict(tickmode=\"linear\", tick0=0, dtick=0.1),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_3_data = pickle.load(open(SAVED_METRICS_PATH / 'standard' / 'proposal_3.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly plot\n",
    "fig = px.line(\n",
    "    pd.DataFrame(proposal_3_data[\"accuracies\"]),\n",
    "    title=\"Accuracy vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Accuracy\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    range_y=[0, 1],\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis=dict(tickmode=\"linear\", tick0=0, dtick=1),\n",
    "    yaxis=dict(tickmode=\"linear\", tick0=0, dtick=0.1),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agem_overall = agem_data[\"accuracies\"][\"Overall\"]\n",
    "cumulative_overall = cumulative_data[\"accuracies\"][\"Overall\"]\n",
    "ewc_overall = ewc_data[\"accuracies\"][\"Overall\"]\n",
    "gem_overall = gem_data[\"accuracies\"][\"Overall\"]\n",
    "lwf_overall = lwf_data[\"accuracies\"][\"Overall\"]\n",
    "mir_overall = mir_data[\"accuracies\"][\"Overall\"]\n",
    "naive_overall = naive[\"accuracies\"][\"Overall\"]\n",
    "offline_overall = offline_data[\"accuracies\"][\"Overall\"] * 10\n",
    "proposal_1_overall = proposal_1_data[\"accuracies\"][\"Overall\"]\n",
    "proposal_2_overall = proposal_2_data[\"accuracies\"][\"Overall\"]\n",
    "proposal_3_overall = proposal_3_data[\"accuracies\"][\"Overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataframe for plotly\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"AGEM\": agem_overall,\n",
    "        \"Cumulative\": cumulative_overall,\n",
    "        \"EWC\": ewc_overall,\n",
    "        \"GEM\": gem_overall,\n",
    "        \"LwF\": lwf_overall,\n",
    "        \"MIR\": mir_overall,\n",
    "        \"Naive\": naive_overall,\n",
    "        \"Offline\": offline_overall,\n",
    "        \"Proposal 1\": proposal_1_overall,\n",
    "        \"Proposal 2\": proposal_2_overall,\n",
    "        \"Proposal 3\": proposal_3_overall,\n",
    "    }\n",
    ")\n",
    "\n",
    "# plotly line\n",
    "fig = px.line(\n",
    "    df,\n",
    "    title=\"Overall Accuracy vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Accuracy\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    range_y=[0.18, 1],\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_layout(\n",
    "    yaxis=dict(tickmode=\"linear\", tick0=0, dtick=0.1),\n",
    "    xaxis=dict(tickmode=\"linear\", tick0=0, dtick=1),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 0 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agem_first_task = agem_data[\"accuracies\"][\"Task0\"]\n",
    "cumulative_first_task = cumulative_data[\"accuracies\"][\"Task0\"]\n",
    "ewc_first_task = ewc_data[\"accuracies\"][\"Task0\"]\n",
    "gem_first_task = gem_data[\"accuracies\"][\"Task0\"]\n",
    "lwf_first_task = lwf_data[\"accuracies\"][\"Task0\"]\n",
    "mir_first_task = mir_data[\"accuracies\"][\"Task0\"]\n",
    "naive_first_task = naive[\"accuracies\"][\"Task0\"]\n",
    "offline_first_task = offline_data[\"accuracies\"][\"Task0\"] * 10\n",
    "proposal_1_first_task = proposal_1_data[\"accuracies\"][\"Task0\"]\n",
    "proposal_2_first_task = proposal_2_data[\"accuracies\"][\"Task0\"]\n",
    "proposal_3_first_task = proposal_3_data[\"accuracies\"][\"Task0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataframe for plotly\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"AGEM\": agem_first_task,\n",
    "        \"Cumulative\": cumulative_first_task,\n",
    "        \"EWC\": ewc_first_task,\n",
    "        \"GEM\": gem_first_task,\n",
    "        \"LwF\": lwf_first_task,\n",
    "        \"MIR\": mir_first_task,\n",
    "        \"Naive\": naive_first_task,\n",
    "        \"Offline\": offline_first_task,\n",
    "        \"Proposal 1\": proposal_1_first_task,\n",
    "        \"Proposal 2\": proposal_2_first_task,\n",
    "        \"Proposal 3\": proposal_3_first_task,\n",
    "    }\n",
    ")\n",
    "\n",
    "# plotly line\n",
    "fig = px.line(\n",
    "    df,\n",
    "    title=\"First Task Accuracy vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Accuracy\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    range_y=[0, 1],\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_layout(\n",
    "    yaxis=dict(tickmode=\"linear\", tick0=0, dtick=0.1),\n",
    "    xaxis=dict(tickmode=\"linear\", tick0=0, dtick=1),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Task Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agem_last_task = agem_data[\"accuracies\"][\"Task9\"]\n",
    "cumulative_last_task = cumulative_data[\"accuracies\"][\"Task9\"]\n",
    "ewc_last_task = ewc_data[\"accuracies\"][\"Task9\"]\n",
    "gem_last_task = gem_data[\"accuracies\"][\"Task9\"]\n",
    "lwf_last_task = lwf_data[\"accuracies\"][\"Task9\"]\n",
    "mir_last_task = mir_data[\"accuracies\"][\"Task9\"]\n",
    "naive_last_task = naive[\"accuracies\"][\"Task9\"]\n",
    "offline_last_task = offline_data[\"accuracies\"][\"Task9\"] * 10\n",
    "proposal_1_last_task = proposal_1_data[\"accuracies\"][\"Task9\"]\n",
    "proposal_2_last_task = proposal_2_data[\"accuracies\"][\"Task9\"]\n",
    "proposal_3_last_task = proposal_3_data[\"accuracies\"][\"Task9\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataframe for plotly\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"AGEM\": agem_last_task,\n",
    "        \"Cumulative\": cumulative_last_task,\n",
    "        \"EWC\": ewc_last_task,\n",
    "        \"GEM\": gem_last_task,\n",
    "        \"LwF\": lwf_last_task,\n",
    "        \"MIR\": mir_last_task,\n",
    "        \"Naive\": naive_last_task,\n",
    "        \"Offline\": offline_last_task,\n",
    "        \"Proposal 1\": proposal_1_last_task,\n",
    "        \"Proposal 2\": proposal_2_last_task,\n",
    "        \"Proposal 3\": proposal_3_last_task,\n",
    "    }\n",
    ")\n",
    "\n",
    "# plotly line\n",
    "fig = px.line(\n",
    "    df,\n",
    "    title=\"Last Task Accuracy vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Accuracy\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    range_y=[0, 1],\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_layout(\n",
    "    yaxis=dict(tickmode=\"linear\", tick0=0, dtick=0.1),\n",
    "    xaxis=dict(tickmode=\"linear\", tick0=0, dtick=1),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd Last Task Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agem_third_last_task = agem_data[\"accuracies\"][\"Task7\"]\n",
    "cumulative_third_last_task = cumulative_data[\"accuracies\"][\"Task7\"]\n",
    "ewc_third_last_task = ewc_data[\"accuracies\"][\"Task7\"]\n",
    "gem_third_last_task = gem_data[\"accuracies\"][\"Task7\"]\n",
    "lwf_third_last_task = lwf_data[\"accuracies\"][\"Task7\"]\n",
    "mir_third_last_task = mir_data[\"accuracies\"][\"Task7\"]\n",
    "naive_third_last_task = naive[\"accuracies\"][\"Task7\"]\n",
    "offline_third_last_task = offline_data[\"accuracies\"][\"Task7\"] * 10\n",
    "proposal_1_third_last_task = proposal_1_data[\"accuracies\"][\"Task7\"]\n",
    "proposal_2_third_last_task = proposal_2_data[\"accuracies\"][\"Task7\"]\n",
    "proposal_3_third_last_task = proposal_3_data[\"accuracies\"][\"Task7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataframe for plotly\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"AGEM\": agem_third_last_task,\n",
    "        \"Cumulative\": cumulative_third_last_task,\n",
    "        \"EWC\": ewc_third_last_task,\n",
    "        \"GEM\": gem_third_last_task,\n",
    "        \"LwF\": lwf_third_last_task,\n",
    "        \"MIR\": mir_third_last_task,\n",
    "        \"Naive\": naive_third_last_task,\n",
    "        \"Offline\": offline_third_last_task,\n",
    "        \"Proposal 1\": proposal_1_third_last_task,\n",
    "        \"Proposal 2\": proposal_2_third_last_task,\n",
    "        \"Proposal 3\": proposal_3_third_last_task,\n",
    "    }\n",
    ")\n",
    "\n",
    "# plotly line\n",
    "fig = px.line(\n",
    "    df,\n",
    "    title=\"Third Last Task Accuracy vs Experience\",\n",
    "    labels=dict(index=\"Experience\", value=\"Accuracy\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    range_y=[0.6, 1],\n",
    "    range_x=[6, 10],\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_layout(\n",
    "    yaxis=dict(tickmode=\"linear\", tick0=0, dtick=0.1),\n",
    "    xaxis=dict(tickmode=\"linear\", tick0=0, dtick=1),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU Usage Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agem_cpu_usage = agem_data[\"train_cpu_usage\"]\n",
    "cumulative_cpu_usage = cumulative_data[\"train_cpu_usage\"]\n",
    "ewc_cpu_usage = ewc_data[\"train_cpu_usage\"]\n",
    "gem_cpu_usage = gem_data[\"train_cpu_usage\"]\n",
    "lwf_cpu_usage = lwf_data[\"train_cpu_usage\"]\n",
    "mir_cpu_usage = mir_data[\"train_cpu_usage\"]\n",
    "naive_cpu_usage = naive[\"train_cpu_usage\"]\n",
    "proposal_1_cpu_usage = proposal_1_data[\"train_cpu_usage\"]\n",
    "proposal_2_cpu_usage = proposal_2_data[\"train_cpu_usage\"]\n",
    "proposal_3_cpu_usage = proposal_3_data[\"train_cpu_usage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"AGEM CPU Usage: {} seconds\".format(\n",
    "        sum(agem_cpu_usage) / len(agem_cpu_usage)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Cumulative CPU Usage: {} seconds\".format(\n",
    "        sum(cumulative_cpu_usage) / len(cumulative_cpu_usage)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"EWC CPU Usage: {} seconds\".format(\n",
    "        sum(ewc_cpu_usage) / len(ewc_cpu_usage)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"GEM CPU Usage: {} seconds\".format(\n",
    "        sum(gem_cpu_usage) / len(gem_cpu_usage)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"LwF CPU Usage: {} seconds\".format(\n",
    "        sum(lwf_cpu_usage) / len(lwf_cpu_usage)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"MIR CPU Usage: {} seconds\".format(\n",
    "        sum(mir_cpu_usage) / len(mir_cpu_usage)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Naive CPU Usage: {} seconds\".format(\n",
    "        sum(naive_cpu_usage) / len(naive_cpu_usage)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Proposal 1 CPU Usage: {} seconds\".format(\n",
    "        sum(proposal_1_cpu_usage) / len(proposal_1_cpu_usage)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Proposal 2 CPU Usage: {} seconds\".format(\n",
    "        sum(proposal_2_cpu_usage) / len(proposal_2_cpu_usage)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Proposal 3 CPU Usage: {} seconds\".format(\n",
    "        sum(proposal_3_cpu_usage) / len(proposal_3_cpu_usage)\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
